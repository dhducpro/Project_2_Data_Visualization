{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b4ca907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting crawl for category 'Science' with depth 1 (max 1000 articles)...\n",
      "Collected 1000 article URLs to scrape.\n",
      "[1/1000] Scraped: List of discoveries influenced by chance circumstances\n",
      "[2/1000] Scraped: ABC@Home\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 215\u001b[39m\n\u001b[32m    213\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    214\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(article_urls)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] Failed to scrape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43muniform\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Be polite to Wikipedia\u001b[39;00m\n\u001b[32m    217\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mScraping complete! Data saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUTPUT_FILE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "\n",
    "# Configuration\n",
    "CATEGORY = \"Science\"\n",
    "DEPTH = 1\n",
    "MAX_ROWS = 1000\n",
    "OUTPUT_FILE = f\"wikipedia_{CATEGORY.lower()}_expanded.csv\"\n",
    "BASE_URL = \"https://en.wikipedia.org\"\n",
    "\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (compatible; WikipediaScraper/1.0; +https://yourdomain.com/bot)'\n",
    "}\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update(HEADERS)\n",
    "\n",
    "def get_soup(url, retries=3, backoff=2):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = session.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            return BeautifulSoup(response.text, 'html.parser')\n",
    "        except (requests.RequestException, requests.Timeout) as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                sleep_time = backoff ** attempt\n",
    "                print(f\"Retrying in {sleep_time} seconds...\")\n",
    "                time.sleep(sleep_time)\n",
    "            else:\n",
    "                print(\"Max retries reached. Skipping this URL.\")\n",
    "                return None\n",
    "\n",
    "def get_first_edit_year(article_url):\n",
    "    # Use MediaWiki API as future improvement (optional)\n",
    "    history_url = article_url + \"?action=history&limit=1&dir=next\"\n",
    "    soup = get_soup(history_url)\n",
    "    if not soup:\n",
    "        return 'Unknown'\n",
    "    history_entry = soup.find('a', class_='mw-changeslist-date')\n",
    "    if history_entry:\n",
    "        match = re.search(r'\\d{4}', history_entry.text)\n",
    "        return match.group(0) if match else 'Unknown'\n",
    "    return 'Unknown'\n",
    "\n",
    "def extract_article_data(article_url):\n",
    "    soup = get_soup(article_url)\n",
    "    if not soup:\n",
    "        return None\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    # Title\n",
    "    title = soup.find('h1', id='firstHeading')\n",
    "    data['Title'] = title.text.strip() if title else 'Unknown'\n",
    "\n",
    "    # Summary (first non-empty paragraph)\n",
    "    content = soup.find('div', class_='mw-parser-output')\n",
    "    summary = None\n",
    "    if content:\n",
    "        for p in content.find_all('p', recursive=False):\n",
    "            text = p.get_text(strip=True)\n",
    "            if text:\n",
    "                summary = text\n",
    "                break\n",
    "    data['Summary'] = summary if summary else 'Unknown'\n",
    "\n",
    "    # Categories\n",
    "    cat_div = soup.find('div', id='mw-normal-catlinks')\n",
    "    if cat_div:\n",
    "        categories = [a.text for a in cat_div.find_all('a')[1:]]  # skip 'Categories'\n",
    "        data['Categories'] = ';'.join(categories) if categories else 'Unknown'\n",
    "    else:\n",
    "        data['Categories'] = 'Unknown'\n",
    "\n",
    "    # References count\n",
    "    refs = soup.find_all('ol', class_='references')\n",
    "    ref_count = 0\n",
    "    if refs:\n",
    "        try:\n",
    "            ref_count = sum(len(ol.find_all('li')) for ol in refs)\n",
    "        except Exception:\n",
    "            ref_count = 0\n",
    "    data['References'] = ref_count\n",
    "\n",
    "    # Internal links count in main content\n",
    "    content_div = soup.find('div', id='mw-content-text')\n",
    "    internal_links_count = 0\n",
    "    if content_div:\n",
    "        internal_links_count = len([a for a in content_div.find_all('a', href=True)\n",
    "                                    if a['href'].startswith('/wiki/') and\n",
    "                                    not a['href'].startswith('/wiki/Category:') and\n",
    "                                    not a['href'].startswith('/wiki/File:')])\n",
    "    data['Links'] = internal_links_count\n",
    "\n",
    "    # External links count in main content\n",
    "    external_links_count = 0\n",
    "    if content_div:\n",
    "        external_links_count = len([a for a in content_div.find_all('a', href=True)\n",
    "                                    if a['href'].startswith('http') and\n",
    "                                    'wikipedia.org' not in a['href']])\n",
    "    data['External_Links'] = external_links_count\n",
    "\n",
    "    # Last edited date\n",
    "    footer = soup.find('li', id='footer-info-lastmod')\n",
    "    if footer and footer.text:\n",
    "        parts = footer.text.split('on ')\n",
    "        if len(parts) > 1:\n",
    "            date_part = parts[1].split(', at')[0].strip()\n",
    "            data['Last_Edited'] = date_part\n",
    "        else:\n",
    "            data['Last_Edited'] = 'Unknown'\n",
    "    else:\n",
    "        data['Last_Edited'] = 'Unknown'\n",
    "\n",
    "    # Word count\n",
    "    word_count = 0\n",
    "    if content:\n",
    "        text = ' '.join(p.get_text() for p in content.find_all('p'))\n",
    "        word_count = len(text.split())\n",
    "    data['Word_Count'] = word_count\n",
    "\n",
    "    # Image count in main content\n",
    "    image_count = 0\n",
    "    if content:\n",
    "        image_count = len(content.find_all('img'))\n",
    "    data['Image_Count'] = image_count\n",
    "\n",
    "    # Section count (h2 and h3 headings in main content)\n",
    "    section_count = 0\n",
    "    if content:\n",
    "        section_count = len(content.find_all(['h2', 'h3']))\n",
    "    data['Section_Count'] = section_count\n",
    "\n",
    "    # First edit year\n",
    "    data['First_Edit_Year'] = get_first_edit_year(article_url)\n",
    "\n",
    "    return data\n",
    "\n",
    "def get_article_urls(category_url, depth, visited_categories=None):\n",
    "    if visited_categories is None:\n",
    "        visited_categories = set()\n",
    "\n",
    "    article_urls = set()\n",
    "    subcategories = []\n",
    "\n",
    "    soup = get_soup(category_url)\n",
    "    if not soup:\n",
    "        return article_urls, subcategories\n",
    "\n",
    "    # Articles in this category\n",
    "    content_div = soup.find('div', id='mw-pages')\n",
    "    if content_div:\n",
    "        for link in content_div.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if href.startswith('/wiki/') and not href.startswith('/wiki/Category:') and not href.startswith('/wiki/File:'):\n",
    "                full_url = BASE_URL + href\n",
    "                article_urls.add(full_url)\n",
    "\n",
    "    # Pagination handling for articles\n",
    "    next_page_link = content_div.find('a', string='next page') if content_div else None\n",
    "    if next_page_link:\n",
    "        next_page_url = BASE_URL + next_page_link['href']\n",
    "        more_articles, more_subcats = get_article_urls(next_page_url, depth, visited_categories)\n",
    "        article_urls.update(more_articles)\n",
    "        subcategories.extend(more_subcats)\n",
    "\n",
    "    # Subcategories\n",
    "    subcat_div = soup.find('div', id='mw-subcategories')\n",
    "    if subcat_div and depth > 0:\n",
    "        for link in subcat_div.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if href.startswith('/wiki/Category:'):\n",
    "                full_url = BASE_URL + href\n",
    "                if full_url not in visited_categories:\n",
    "                    visited_categories.add(full_url)\n",
    "                    subcategories.append(full_url)\n",
    "\n",
    "        # For each subcategory, recurse only if depth allows\n",
    "        for subcat_url in list(subcategories):\n",
    "            if depth > 0:\n",
    "                sub_articles, _ = get_article_urls(subcat_url, depth - 1, visited_categories)\n",
    "                article_urls.update(sub_articles)\n",
    "\n",
    "    return article_urls, subcategories\n",
    "\n",
    "def crawl_category(category, depth):\n",
    "    category_url = f\"{BASE_URL}/wiki/Category:{category.replace(' ', '_')}\"\n",
    "    article_urls, _ = get_article_urls(category_url, depth)\n",
    "    # Limit and return as list\n",
    "    return list(article_urls)[:MAX_ROWS]\n",
    "\n",
    "\n",
    "print(f\"Starting crawl for category '{CATEGORY}' with depth {DEPTH} (max {MAX_ROWS} articles)...\")\n",
    "article_urls = crawl_category(CATEGORY, DEPTH)\n",
    "print(f\"Collected {len(article_urls)} article URLs to scrape.\")\n",
    "\n",
    "with open(OUTPUT_FILE, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['Title', 'Summary', 'Categories', 'References', 'Links', 'Last_Edited',\n",
    "                  'Word_Count', 'Image_Count', 'Section_Count', 'External_Links', 'First_Edit_Year']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for i, url in enumerate(article_urls):\n",
    "        data = extract_article_data(url)\n",
    "        if data:\n",
    "            writer.writerow(data)\n",
    "            print(f\"[{i+1}/{len(article_urls)}] Scraped: {data['Title']}\")\n",
    "        else:\n",
    "            print(f\"[{i+1}/{len(article_urls)}] Failed to scrape: {url}\")\n",
    "        time.sleep(random.uniform(1, 2))  # Be polite to Wikipedia\n",
    "\n",
    "print(f\"Scraping complete! Data saved to {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a11434-1b73-4efa-a400-968f2816d518",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
